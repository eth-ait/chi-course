{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Overview #\n",
    "This notebook walks you through some simple but general key-ideas in machine learning. In particular we will perform the following steps:\n",
    "\n",
    "   * Import or create a dataset\n",
    "   * Visualize the dataset\n",
    "   * Train a simple classifier on the dataset\n",
    "   * Make predictions with the trained classifier\n",
    "\n",
    "Furthermore, we will investigate what happens when the data becomes harder.  Specifically we are looking at: \n",
    "   * ways to deal with non-linearly seperable data.  \n",
    "   * ways to deal with underfitting\n",
    "   * ways to deal with overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Import NumPy and Scikit-learn. \n",
    "# See http://scikit-learn.org/stable/user_guide.html for many more tutorials\n",
    "import time\n",
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "# Import matlibplot and seaborn for data visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# <!-- collapse=True -->\n",
    "\n",
    "# Modified from http://scikit-learn.org/stable/auto_examples/plot_learning_curve.html\n",
    "from sklearn.learning_curve import learning_curve\n",
    "def plot_learning_curve(estimator, title, X, y, ylim=None, cv=None,\n",
    "                        train_sizes=np.linspace(.1, 1.0, 5)):\n",
    "    \"\"\"\n",
    "    Generate a simple plot of the test and traning learning curve.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    estimator : object type that implements the \"fit\" and \"predict\" methods\n",
    "        An object of that type which is cloned for each validation.\n",
    "\n",
    "    title : string\n",
    "        Title for the chart.\n",
    "\n",
    "    X : array-like, shape (n_samples, n_features)\n",
    "        Training vector, where n_samples is the number of samples and\n",
    "        n_features is the number of features.\n",
    "\n",
    "    y : array-like, shape (n_samples) or (n_samples, n_features), optional\n",
    "        Target relative to X for classification or regression;\n",
    "        None for unsupervised learning.\n",
    "\n",
    "    ylim : tuple, shape (ymin, ymax), optional\n",
    "        Defines minimum and maximum yvalues plotted.\n",
    "\n",
    "    cv : integer, cross-validation generator, optional\n",
    "        If an integer is passed, it is the number of folds (defaults to 3).\n",
    "        Specific cross-validation objects can be passed, see\n",
    "        sklearn.cross_validation module for the list of possible objects\n",
    "    \"\"\"\n",
    "    \n",
    "    plt.figure()\n",
    "    train_sizes, train_scores, test_scores = learning_curve(\n",
    "        estimator, X, y, cv=5, n_jobs=1, train_sizes=train_sizes)\n",
    "    train_scores_mean = np.mean(train_scores, axis=1)\n",
    "    train_scores_std = np.std(train_scores, axis=1)\n",
    "    test_scores_mean = np.mean(test_scores, axis=1)\n",
    "    test_scores_std = np.std(test_scores, axis=1)\n",
    "\n",
    "    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n",
    "                     train_scores_mean + train_scores_std, alpha=0.1,\n",
    "                     color=\"r\")\n",
    "    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n",
    "                     test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n",
    "    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n",
    "             label=\"Training score\")\n",
    "    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n",
    "             label=\"Cross-validation score\")\n",
    "\n",
    "    plt.xlabel(\"Training examples\")\n",
    "    plt.ylabel(\"Score\")\n",
    "    plt.legend(loc=\"best\")\n",
    "    plt.grid(\"on\") \n",
    "    if ylim:\n",
    "        plt.ylim(ylim)\n",
    "    plt.title(title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# <!-- collapse=True -->\n",
    "\n",
    "def plot_model_parameters(estimator,title,X,y):\n",
    "    \"\"\"\n",
    "      Plots decision surface, support vectors and margins for SVM classifiers\n",
    "    \"\"\"\n",
    "    # get the separating hyperplane\n",
    "    w = estimator.coef_[0]\n",
    "    a = -w[0] / w[1]\n",
    "    xx = np.linspace(-5, 5)\n",
    "    yy = a * xx - (estimator.intercept_[0]) / w[1]\n",
    "\n",
    "    # plot the parallels to the separating hyperplane that pass through the\n",
    "    # support vectors\n",
    "    b = estimator.support_vectors_[0]\n",
    "    yy_down = a * xx + (b[1] - a * b[0])\n",
    "    b = estimator.support_vectors_[-1]\n",
    "    yy_up = a * xx + (b[1] - a * b[0])\n",
    "\n",
    "    # plot the line, the points, and the nearest vectors to the plane\n",
    "    plt.plot(xx, yy, 'k-')\n",
    "    plt.plot(xx, yy_down, 'k--')\n",
    "    plt.plot(xx, yy_up, 'k--')\n",
    "\n",
    "    plt.scatter(estimator.support_vectors_[:, 0], estimator.support_vectors_[:, 1],\n",
    "            s=90, facecolors='black')\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.Paired)\n",
    "\n",
    "\n",
    "    plt.axis('tight')\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "    \n",
    "def plot_data(X,y):\n",
    "    plt.scatter(X[:,0], X[:,1], c=y, cmap=plt.cm.Paired)\n",
    "\n",
    "    plt.axis('tight')\n",
    "    plt.show()\n",
    "    \n",
    "def plot_model_parameters_non_linear(estimator,title,X,y): \n",
    "    h = .02  # step size in the mesh\n",
    "    \n",
    "    # create a mesh to plot in\n",
    "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                     np.arange(y_min, y_max, h))\n",
    "    \n",
    "    Z = estimator.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    # Put the result into a color plot\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    plt.contourf(xx, yy, Z, cmap=plt.cm.coolwarm, alpha=0.8)\n",
    "\n",
    "    # Plot also the training points\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.coolwarm)\n",
    "    plt.xlim(xx.min(), xx.max())\n",
    "    plt.ylim(yy.min(), yy.max())\n",
    "    plt.xticks(())\n",
    "    plt.yticks(())\n",
    "    plt.title(title)\n",
    "    plt.axis('tight')\n",
    "    \n",
    "\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data #\n",
    "\n",
    "First, we will generate some simple toy data. \n",
    "\n",
    "Using [sklearn](http://scikit-learn.org)'s `make_classification` and `make_blob` functions you have more control over the type of data and its distribution. \n",
    "\n",
    "**Practical tip:** sklearn has many useful helper functions to deal with real data and data pre-processing (http://scikit-learn.org/stable/data_transforms.html) that can be used in real-world settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# we create 100 separable points\n",
    "np.random.seed(0)\n",
    "X = np.r_[np.random.randn(20, 2) - [2, 2], np.random.randn(20, 2) + [2, 2]]\n",
    "Y = [0] * 20 + [1] * 20\n",
    "\n",
    "plot_data(X,Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training\n",
    "\n",
    "Now let's create and train a linear SVM on this data. The procedure shown here is the same for almost all ML estimators in sklearn:\n",
    "\n",
    "   * create an estimator instance\n",
    "   * train the estimator via its .fit() function\n",
    "   * make predictions via the .predict() function\n",
    "   * you can typically access the learned model parameters via the .coeff_ field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# import SVM module\n",
    "from sklearn import svm\n",
    "\n",
    "# create and fit the model\n",
    "clf = svm.SVC(kernel='linear')\n",
    "clf.fit(X, Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizing Model Parameters\n",
    "\n",
    "Now we will look at the learned model. Or rather the estimated model parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_model_parameters(clf,\"linear SVM\",X,Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Non Linearily Seperable Data\n",
    "\n",
    "This ideal case of two completely non-overlapping clusters is of course not realistic. Now we will look at some more realistic data.\n",
    "\n",
    "Again we synthesize some data and will train a model on this. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "X, y = make_classification(100, n_features=2, n_informative=2, \n",
    "                           n_redundant=0, n_classes=2, random_state=0)\n",
    "\n",
    "\n",
    "plot_data(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# create and fit the model\n",
    "clf = svm.SVC(kernel='linear')\n",
    "clf.fit(X, y)\n",
    "#let's see how well that worked\n",
    "plot_model_parameters(clf,\"linear SVM\",X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That didn't work so well. Let's try an even harder case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_circles\n",
    "X,y = make_circles(100,2)\n",
    "\n",
    "plot_data(X,y)\n",
    "\n",
    "# create and fit the model\n",
    "clf = svm.SVC(kernel='linear')\n",
    "clf.fit(X, y)\n",
    "#let's see how well that worked\n",
    "plot_model_parameters(clf,\"linear SVM\",X,y)\n",
    "\n",
    "#Also let's look at learning and validation error\n",
    "plot_learning_curve(clf, \"linear SVM\", \n",
    "                    X, y, ylim=(0.5, 1.0),\n",
    "                    train_sizes=np.linspace(.1, 1.0, 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uh-Oh - that didn't work at all. What now? (Let's go back to slides)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kernel Trick\n",
    "In the above example the model is _underfitting_ (it performs bad both on test and training data). We need to use a model with higher model capacity. For this we will use kernel SVMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "clf=SVC(C=2.5, kernel=\"rbf\", gamma=1.0)\n",
    "clf.fit(X,y)\n",
    "\n",
    "plot_model_parameters_non_linear(clf,\"SVC(C=2.5, kernel='rbf', gamma=1.0)\",X,y)\n",
    "\n",
    "plot_learning_curve(clf,\n",
    "                    \"SVC(C=2.5, kernel='rbf', gamma=1.0)\",\n",
    "                    X, y, ylim=(0.5, 1.0), \n",
    "                    train_sizes=np.linspace(.1, 1.0, 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A more realistic example\n",
    "We are now going to look at a more realistic example. The key idea will now be how to control overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X, y = make_classification(1000, n_features=20, n_informative=2, \n",
    "                           n_redundant=2, n_classes=2, random_state=0)\n",
    "\n",
    "from pandas import DataFrame\n",
    "df = DataFrame(np.hstack((X, y[:, None])), columns = list(range(20)) + [\"class\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data visualization\n",
    "The first step in building an ML pipeline is to understand the data. The table above helps but it's hard to glean insights. We'll use [Seaborn](http://stanford.edu/~mwaskom/software/seaborn/) to vizualize our data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "_ = sns.pairplot(df[:50], vars=[8, 11, 12, 14, 19], hue=\"class\", size=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that some features (11&14) are more helpful to distinguish the two classes than others _(study the call to make_classification to understand why)_.  \n",
    "Feature 12 and 19 are highly anti-correlated. \n",
    "\n",
    "We can explore correlations more systematically by using corrplot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Compute the correlation matrix\n",
    "corr = df.corr()\n",
    "\n",
    "sns.set(style=\"white\")\n",
    "\n",
    "# Generate a mask for the upper triangle\n",
    "mask = np.zeros_like(corr, dtype=np.bool)\n",
    "mask[np.triu_indices_from(mask)] = True\n",
    "\n",
    "#plotting\n",
    "f, ax = plt.subplots(figsize=(11, 9))\n",
    "# Generate a custom diverging colormap\n",
    "cmap = sns.diverging_palette(220, 10, as_cmap=True)\n",
    "\n",
    "# Draw the heatmap with the mask and correct aspect ratio\n",
    "sns.heatmap(corr, mask=mask, cmap=cmap, vmax=.3,\n",
    "            square=True, xticklabels=5, yticklabels=5,\n",
    "            linewidths=.5, cbar_kws={\"shrink\": .5}, ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model selection\n",
    "\n",
    "We'll first try the linear SVM again. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "plot_learning_curve(LinearSVC(C=10.0), \"LinearSVC(C=10.0)\",\n",
    "                    X, y, ylim=(0.8, 1.01),\n",
    "                    train_sizes=np.linspace(.05, 0.2, 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The learning curve shows a huge gap between performance on training data and the performance on test data. This model is not going to perform well on real data - the model is _overfitting_\n",
    "\n",
    "# Controlling overfitting\n",
    "\n",
    "There are different ways to decreasing overfitting:\n",
    "\n",
    "   * **more training examples** (getting more data is a common wish in ML)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_learning_curve(LinearSVC(C=10.0), \"LinearSVC(C=10.0)\",\n",
    "                    X, y, ylim=(0.8, 1.1),\n",
    "                    train_sizes=np.linspace(.1, 1.0, 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That looks better! \n",
    "\n",
    "**Unfortunately getting more training data is often costly. Especially in HCI (where this involves getting humans do do things) it is often not feasible to collect sufficient data.**\n",
    "\n",
    "An alternative would be:\n",
    "\n",
    " * **decrease the number of features** (we know that f11 & f14 are most informative)\n",
    " \n",
    "Let's use those informative features. However, note that this is 'cheating' in the real-world finding informative features is much harder.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_learning_curve(LinearSVC(C=10.0), \"LinearSVC(C=10.0) Features: 11&14\",\n",
    "                    X[:, [11, 14]], y, ylim=(0.8, 1.0),\n",
    "                    train_sizes=np.linspace(.05, 0.2, 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try an automatic approach:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "# SelectKBest(f_classif, k=2) will select the k=2 best features according to their Anova F-value\n",
    "\n",
    "plot_learning_curve(Pipeline([(\"fs\", SelectKBest(f_classif, k=2)), # select two features\n",
    "                               (\"svc\", LinearSVC(C=10.0))]),\n",
    "                    \"SelectKBest(f_classif, k=2) + LinearSVC(C=10.0)\",\n",
    "                    X, y, ylim=(0.8, 1.0),\n",
    "                    train_sizes=np.linspace(.05, 0.2, 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This worked remarkably well. Feature selection is simple on this toy data. **Again: with real data this is a much harder problem!**\n",
    "\n",
    "# Model parameters\n",
    "Another aspect of interest are the parameters of the classifier. In SVMs this would be the regularizition and Kernel parameters. Finding the best parameters here can turn into a wild goose chase if done manually. \n",
    "\n",
    "The best way to do this is via **grid-search** and **k-fold cross-valditation**. Luckily this is built into sklear:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.grid_search import GridSearchCV\n",
    "\n",
    "est = GridSearchCV(LinearSVC(), \n",
    "                   param_grid={\"C\": [0.001, 0.01, 0.1, 1.0, 10.0]})\n",
    "plot_learning_curve(est, \"LinearSVC(C=AUTO)\", \n",
    "                    X, y, ylim=(0.8, 1.0),\n",
    "                    train_sizes=np.linspace(.05, 0.2, 5))\n",
    "print(\"Chosen parameter on 100 datapoints: %s\" % est.fit(X[:100], y[:100]).best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general, feature selection looked better. \n",
    "\n",
    "# L1-Norm\n",
    "LinearSVC also supports the l1 penalty, which results in sparse solutions. Sparse solutions correspond to an implicit feature selection. Let's try this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_learning_curve(LinearSVC(C=0.1, penalty='l1', dual=False), \n",
    "                    \"LinearSVC(C=0.1, penalty='l1')\", \n",
    "                    X, y, ylim=(0.8, 1.0),\n",
    "                    train_sizes=np.linspace(.05, 0.2, 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This also looks quite well! \n",
    "\n",
    "# Summary\n",
    "Let's stop here and summarize:\n",
    "\n",
    "   * We have discussed some recommendations of how to get machine learning working on a new problem. \n",
    "   * We have looked at classification problems but regression and clustering can be addressed similarly. \n",
    "   * Hoewever, the focus on artificial datasets was (while being easily to understand) also somewhat oversimplifying. \n",
    "   * On many actual problems, the collection, organisation, and preprocessing of the data are of uttermost importance.\n",
    "   * In many HCI problems signal processing and feature selection domainate the process. However, there is a big push in ML towards learning the whole pipeline in an end-to-end fashion via deep neural networks. This is an advanced topic and is left for next time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
